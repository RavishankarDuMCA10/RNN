{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d2e958",
   "metadata": {},
   "source": [
    "# POS Tagging - An Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09800bf",
   "metadata": {},
   "source": [
    "The process of classifying words into their __parts of speech__ and labeling them accordingly is known as **part-of-speech tagging**, or simply **POS-tagging**.\n",
    "\n",
    "The NLTK library has a number of corpora which contains word and its POS tag. The following table provide information about each tag:\n",
    "\n",
    "![POS tags](./jupyter resources/pos_tagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04905da",
   "metadata": {},
   "source": [
    "# Notebook layout\n",
    "1. Preprocess data\n",
    "2. Vanilla RNN\n",
    "3. Word Embeddings\n",
    "4. LSTM\n",
    "5. GRU\n",
    "6. Bidirectional LSTM\n",
    "7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa70421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: keras in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (2.13.1)\n",
      "Requirement already satisfied: gensim in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Obtaining dependency information for FuzzyTM>=0.4.0 from https://files.pythonhosted.org/packages/2d/30/074bac7a25866a2807c1005c7852c0139ac22ba837871fc01f16df29b9dc/FuzzyTM-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for pyfume from https://files.pythonhosted.org/packages/ed/ea/a3b120e251145dcdb10777f2bc5f18b1496fd999d705a178c1b0ad947ce1/pyFUME-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Obtaining dependency information for numpy>=1.18.5 from https://files.pythonhosted.org/packages/c0/bc/77635c657a3668cf652806210b8662e1aff84b818a55ba88257abf6637a8/numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Obtaining dependency information for simpful==2.12.0 from https://files.pythonhosted.org/packages/9d/0e/aebc2fb0b0f481994179b2ee2b8e6bbf0894d971594688c018375e7076ea/simpful-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=fc2156f24a12ebae633532047f48bfb9b531063b2ebc307a82149384fb25cdd0\n",
      "  Stored in directory: /Users/ravishankarkushwaha/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=5929abdacbd2f2705aa28dd63f32a15f58e28c037b51158bfe49c99d87007893\n",
      "  Stored in directory: /Users/ravishankarkushwaha/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: numpy, simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "tables 3.8.0 requires cython>=0.29.21, which is not installed.\n",
      "pennylane 0.29.1 requires numpy<1.24, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow-macos 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n",
      "retworkx 0.13.1 requires rustworkx==0.13.1, but you have rustworkx 0.15.1 which is incompatible.\n",
      "qiskit-ibmq-provider 0.20.2 requires numpy<1.24, but you have numpy 1.24.4 which is incompatible.\n",
      "xanadu-cloud-client 0.3.1 requires pydantic[dotenv]<2, but you have pydantic 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 miniful-0.0.6 numpy-1.24.4 pyfume-0.3.4 simpful-2.12.0\n",
      "Requirement already satisfied: seaborn in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ravishankarkushwaha/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "## Install importent libraries\n",
    "!pip install nltk\n",
    "!pip install keras\n",
    "!pip install gensim\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94483056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd23c36",
   "metadata": {},
   "source": [
    "## 1. Preprocessing data\n",
    "\n",
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69a8d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/ravishankarkushwaha/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/ravishankarkushwaha/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/ravishankarkushwaha/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/ravishankarkushwaha/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "\n",
    "# load POS tagged corpora from NLTK\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus+brown_corpus+conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7424772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '.'),\n",
       " ('We', 'PRON'),\n",
       " ('have', 'VERB'),\n",
       " ('no', 'DET'),\n",
       " ('useful', 'ADJ'),\n",
       " ('information', 'NOUN'),\n",
       " ('on', 'ADP'),\n",
       " ('whether', 'ADP'),\n",
       " ('users', 'NOUN'),\n",
       " ('are', 'VERB'),\n",
       " ('at', 'ADP'),\n",
       " ('risk', 'NOUN'),\n",
       " (',', '.'),\n",
       " (\"''\", '.'),\n",
       " ('said', 'VERB'),\n",
       " ('*T*-1', 'X'),\n",
       " ('James', 'NOUN'),\n",
       " ('A.', 'NOUN'),\n",
       " ('Talcott', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('Boston', 'NOUN'),\n",
       " (\"'s\", 'PRT'),\n",
       " ('Dana-Farber', 'NOUN'),\n",
       " ('Cancer', 'NOUN'),\n",
       " ('Institute', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the data\n",
    "tagged_sentences[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ca82",
   "metadata": {},
   "source": [
    "## Divide data in words (X) and tags (Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c856d",
   "metadata": {},
   "source": [
    "Since this is a **many-to-many** problem, each data point will be a different sentence of the corpora.\n",
    "\n",
    "Each data point will have multiple words in the **input sequence**. This is what we will refer to as **X**.\n",
    "\n",
    "Each word will have its correpsonding tag in the **output sequence**. This what we will refer to as **Y**.\n",
    "\n",
    "Sample dataset:\n",
    "\n",
    "|                    X                        |                 Y                |\n",
    "|---------------------------------------------|----------------------------------|\n",
    "|   Mr. Vinken is chairman of Elsevier        |   NOUN NOUN VERB NOUN ADP NOUN   |\n",
    "|     We have no useful information           |      PRON VERB DET ADJ NOUN      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f55f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8a25786",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "141a0057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tagged sentences: 72202\n",
      "Vocabulary size: 59448\n",
      "Total number of tags: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of tagged sentences: {}\".format(len(X)))\n",
    "print(\"Vocabulary size: {}\".format(num_words))\n",
    "print(\"Total number of tags: {}\".format(num_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f87207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "sample Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at first data point\n",
    "# this is one data point that will be fed to the RNN\n",
    "print('sample X: ', X[0], '\\n')\n",
    "print('sample Y: ', Y[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c1f822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first input sequence  : 18\n",
      "Length of first output sequence : 18\n"
     ]
    }
   ],
   "source": [
    "# In this many-to-many problem, the length of each input and output sequence must be the same.\n",
    "# Since each word is tagged, it's important to make sure that the length of input sequence equals the output sequence\n",
    "print(\"Length of first input sequence  : {}\".format(len(X[0])))\n",
    "print(\"Length of first output sequence : {}\".format(len(Y[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c6974",
   "metadata": {},
   "source": [
    "## Vectorise X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0059162",
   "metadata": {},
   "source": [
    "#### Encode X and Y to integer values\n",
    "\n",
    "We'll use the Tokenizer() function from Keras library to encode text sequence to integer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1999ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode X\n",
    "\n",
    "word_tokenizer = Tokenizer()                      # instantiate tokeniser\n",
    "word_tokenizer.fit_on_texts(X)                    # fit tokeniser on data\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X)  # use the tokeniser to encode input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b37996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode Y\n",
    "\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(Y)\n",
    "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9cf27be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Raw data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
      "\n",
      "Y:  ['NOUN', 'NOUN', '.', 'NUM', 'NOUN', 'ADJ', '.', 'VERB', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NUM', '.'] \n",
      "\n",
      "\n",
      "** Encoded data point ** \n",
      " ---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "X:  [6423, 24231, 2, 7652, 102, 170, 2, 47, 1898, 1, 269, 17, 7, 13230, 619, 1711, 2761, 3] \n",
      "\n",
      "Y:  [1, 1, 3, 11, 1, 6, 3, 2, 2, 5, 1, 4, 5, 6, 1, 1, 11, 3] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at first encoded data point\n",
    "\n",
    "print(\"** Raw data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X[0], '\\n')\n",
    "print('Y: ', Y[0], '\\n')\n",
    "print()\n",
    "print(\"** Encoded data point **\", \"\\n\", \"-\"*100, \"\\n\")\n",
    "print('X: ', X_encoded[0], '\\n')\n",
    "print('Y: ', Y_encoded[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99800b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentences have disparate input-output lengths.\n"
     ]
    }
   ],
   "source": [
    "# make sure that each sequence of input and output is same length\n",
    "\n",
    "different_length = [1 if len(input) != len(output) else 0 for input, output in zip(X_encoded, Y_encoded)]\n",
    "print(\"{} sentences have disparate input-output lengths.\".format(sum(different_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23704078",
   "metadata": {},
   "source": [
    "## Pad sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f7276",
   "metadata": {},
   "source": [
    "The next step after encoding the data is to **define the sequence lengths**. As of now, the sentences present in the data are of various lengths. We need to either pad short sentences or truncate long sentences to a fixed length. This fixed length, however, is a **hyperparameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d80020dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of longest sentence: 271\n"
     ]
    }
   ],
   "source": [
    "# check length of longest sentence\n",
    "lengths = [len(seq) for seq in X_encoded]\n",
    "print(\"Length of longest sentence: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e17f0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhZUlEQVR4nO3df2xV9R3/8ddpS+8FbBugcttrf8gWWIxlfjN+FDtUGFhsBCIuo9/VGUmYPyYQaiEqM8vAGDpdViAyiSTGAlLhH1ET6UaZgrI7pTbyFdwiLAHahpbODm9b1nsL7fn+YbjhQgHLhX4+vff5SG5Cz+e0vG9I6bPnnnuO47quKwAAAIskmR4AAADgUgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOukmB7gevT19enUqVNKS0uT4zimxwEAAN+D67rq7OyU3+9XUtLVj5EMyUA5deqUcnNzTY8BAACuQ1NTk3Jycq66z5AMlLS0NEnfPcH09HTD0wAAgO+jo6NDubm5kZ/jVzMkA+XCyzrp6ekECgAAQ8z3OT2Dk2QBAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAFglEAiotLRUgUDA9CgADCJQAFgjFAqpqqpKp0+fVlVVlUKhkOmRABhCoACwxvbt29Xe3i5Jam9vV01NjeGJAJhCoACwQnNzs2pqauS6rqTvbsteU1Oj5uZmw5MBMIFAAWCc67rasGHDFbdfiBYAiYNAAWBcY2Oj6uvr1dvbG7W9t7dX9fX1amxsNDQZAFMIFADG5eXlacqUKUpOTo7anpycrKlTpyovL8/QZABMIVAAGOc4jpYvX37F7Y7jGJgKgEkECgAr5OTkqKysLBIjjuOorKxMt912m+HJAJhAoACwxiOPPKIxY8ZIkjIzM1VWVmZ4IgCmECgArOH1elVRUSGfz6dnnnlGXq/X9EgADEkxPQAAXKyoqEhFRUWmxwBgGEdQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1iFQAACAdQgUAABgHQIFAABYZ0CBUllZqSlTpigtLU1jx47VQw89pK+//jpqn0WLFslxnKjHtGnTovYJh8NatmyZMjMzNXLkSM2fP1/Nzc2xPxsAABAXBhQo+/fv15IlS/Tpp5+qrq5O58+fV3Fxsc6ePRu13wMPPKCWlpbIY/fu3VHr5eXl2rVrl3bs2KEDBw6oq6tLc+fOVW9vb+zPCAAADHkpA9n5L3/5S9THb775psaOHauGhgbde++9ke0ej0dZWVn9fo1gMKg33nhD27Zt0+zZsyVJb731lnJzc7V3717NmTNnoM8BAADEmZjOQQkGg5Kk0aNHR23ft2+fxo4dqwkTJujxxx9XW1tbZK2hoUHnzp1TcXFxZJvf71dBQYECgUAs4wAAgDgxoCMoF3NdVxUVFZo+fboKCgoi20tKSvSLX/xC+fn5On78uH73u9/pZz/7mRoaGuTxeNTa2qrU1FSNGjUq6uv5fD61trb2+3eFw2GFw+HIxx0dHdc7NgAAGAKuO1CWLl2qL7/8UgcOHIjaXlpaGvlzQUGBJk+erPz8fH3wwQd6+OGHr/j1XNeV4zj9rlVWVmrNmjXXOyoAABhiruslnmXLlun999/XRx99pJycnKvum52drfz8fB07dkySlJWVpZ6eHp05cyZqv7a2Nvl8vn6/xqpVqxQMBiOPpqam6xkbAAAMEQMKFNd1tXTpUr3zzjv68MMPNW7cuGt+Tnt7u5qampSdnS1JmjRpkoYNG6a6urrIPi0tLTpy5IiKior6/Roej0fp6elRDwAAEL8G9BLPkiVLVFNTo/fee09paWmRc0YyMjI0fPhwdXV1afXq1fr5z3+u7OxsnThxQr/97W+VmZmpBQsWRPZdvHixVqxYoTFjxmj06NFauXKlJk6cGHlXDwAASGwDCpRNmzZJkmbMmBG1/c0339SiRYuUnJysw4cPa+vWrfr222+VnZ2tmTNnaufOnUpLS4vsv27dOqWkpGjhwoXq7u7WrFmzVF1dreTk5NifEQAAGPIc13Vd00MMVEdHhzIyMhQMBnm5BwCAIWIgP7+5Fw8AALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoDCpTKykpNmTJFaWlpGjt2rB566CF9/fXXUfu4rqvVq1fL7/dr+PDhmjFjhr766quofcLhsJYtW6bMzEyNHDlS8+fPV3Nzc+zPBgAAxIUBBcr+/fu1ZMkSffrpp6qrq9P58+dVXFyss2fPRvZ55ZVXVFVVpY0bN6q+vl5ZWVm6//771dnZGdmnvLxcu3bt0o4dO3TgwAF1dXVp7ty56u3tvXHPDAAADFmO67ru9X7yf/7zH40dO1b79+/XvffeK9d15ff7VV5erueee07Sd0dLfD6fXn75ZT355JMKBoO69dZbtW3bNpWWlkqSTp06pdzcXO3evVtz5sy55t/b0dGhjIwMBYNBpaenX+/4AABgEA3k53dM56AEg0FJ0ujRoyVJx48fV2trq4qLiyP7eDwe3XfffQoEApKkhoYGnTt3Lmofv9+vgoKCyD6XCofD6ujoiHoAAID4dd2B4rquKioqNH36dBUUFEiSWltbJUk+ny9qX5/PF1lrbW1VamqqRo0adcV9LlVZWamMjIzIIzc393rHBgAAQ8B1B8rSpUv15Zdf6u23375szXGcqI9d171s26Wuts+qVasUDAYjj6ampusdGwAADAHXFSjLli3T+++/r48++kg5OTmR7VlZWZJ02ZGQtra2yFGVrKws9fT06MyZM1fc51Iej0fp6elRDwAAEL8GFCiu62rp0qV655139OGHH2rcuHFR6+PGjVNWVpbq6uoi23p6erR//34VFRVJkiZNmqRhw4ZF7dPS0qIjR45E9gEAAIktZSA7L1myRDU1NXrvvfeUlpYWOVKSkZGh4cOHy3EclZeXa+3atRo/frzGjx+vtWvXasSIESorK4vsu3jxYq1YsUJjxozR6NGjtXLlSk2cOFGzZ8++8c8QAAAMOQMKlE2bNkmSZsyYEbX9zTff1KJFiyRJzz77rLq7u/X000/rzJkzKiws1J49e5SWlhbZf926dUpJSdHChQvV3d2tWbNmqbq6WsnJybE9GwAAEBdiug6KKVwHBQCAoWfQroMCAABwMxAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAKwSCARUWlqqQCBgehQABhEoAKwRCoVUVVWl06dPq6qqSqFQyPRIAAwhUABYY/v27Wpvb5cktbe3q6amxvBEAEwhUABYobm5WTU1NXJdV5Lkuq5qamrU3NxseDIAJhAoAIxzXVcbNmy44vYL0QIgcRAoAIxrbGxUfX29ent7o7b39vaqvr5ejY2NhiYDYAqBAsC4vLw8TZkyRcnJyVHbk5OTNXXqVOXl5RmaDIApBAoA4xzH0fLly6+43XEcA1MBMIlAAWCFnJwclZWVRWLEcRyVlZXptttuMzwZABMIFADWeOSRRzRmzBhJUmZmpsrKygxPBMAUAgWANbxer0pKSpSUlKQHHnhAXq/X9EgADCFQAFgjFAqptrZWfX19qq2t5UqyQAIjUABYgyvJAriAQAFgBa4kC+BiBAoA47iSLIBLESgAjONKsgAuRaAAMI4ryQK4FIECwDiuJAvgUgQKACtwJVkAFyNQAFiDK8kCuIBAAWANr9eriooK+Xw+PfPMM1xJFkhgKaYHAICLFRUVqaioyPQYAAzjCAoAALAOgQLAKoFAQKWlpQoEAqZHAWAQgQLAGqFQSFVVVTp9+rSqqqq4WSCQwAgUANbgZoEALiBQAFiBmwUCuBiBAsA4bhYI4FIECgDjuFkggEsRKACM42aBAC5FoAAwjpsFArgUgQLACtwsEMDFCBQA1uBmgQAuIFAAWIObBQK4gJsFArAKNwsEIHEEBQAAWIhAAQAA1iFQAACAdQYcKB9//LHmzZsnv98vx3H07rvvRq0vWrRIjuNEPaZNmxa1Tzgc1rJly5SZmamRI0dq/vz53G8DAABEDDhQzp49q7vuuksbN2684j4PPPCAWlpaIo/du3dHrZeXl2vXrl3asWOHDhw4oK6uLs2dO/eyy1wDAIDENOB38ZSUlKikpOSq+3g8HmVlZfW7FgwG9cYbb2jbtm2aPXu2JOmtt95Sbm6u9u7dqzlz5gx0JAAAEGduyjko+/bt09ixYzVhwgQ9/vjjamtri6w1NDTo3LlzKi4ujmzz+/0qKChQIBDo9+uFw2F1dHREPQAAQPy64YFSUlKi7du368MPP9Sf/vQn1dfX62c/+5nC4bAkqbW1VampqRo1alTU5/l8PrW2tvb7NSsrK5WRkRF55Obm3uixAVgiEAiotLT0ir+wAEgMNzxQSktL9eCDD6qgoEDz5s1TbW2tjh49qg8++OCqn+e67hVvCLZq1SoFg8HIo6mp6UaPDcACoVBIlZWVOn36tCorKxUKhUyPBMCQm/424+zsbOXn5+vYsWOSpKysLPX09OjMmTNR+7W1tcnn8/X7NTwej9LT06MeAOLPli1b1NnZKUnq7OzU1q1bDU8EwJSbHijt7e1qampSdna2JGnSpEkaNmyY6urqIvu0tLToyJEjXN4aSGDNzc3asWNH1La3336bSxAACWrAgdLV1aVDhw7p0KFDkqTjx4/r0KFDamxsVFdXl1auXKl//OMfOnHihPbt26d58+YpMzNTCxYskCRlZGRo8eLFWrFihf72t7/piy++0K9+9StNnDgx8q4eAInFdV29/PLLcl33e20HEP8G/Dbjzz//XDNnzox8XFFRIUl67LHHtGnTJh0+fFhbt27Vt99+q+zsbM2cOVM7d+5UWlpa5HPWrVunlJQULVy4UN3d3Zo1a5aqq6uVnJx8A54SgKHm5MmTOnz4cL9rhw8f1smTJ3X77bcP7lAAjBpwoMyYMeOqv8389a9/vebX8Hq9evXVV/Xqq68O9K8HAAAJgHvxADAuNzdXSUn9/3eUlJTEpQWABESgADDus88+U19fX79rfX19+uyzzwZ5IgCmESgAjLvwLr/rXQcQfwgUAMbl5eVd8UKNjuMoLy9vkCcCYBqBAsC4gwcPXvHke9d1dfDgwUGeCIBpBAoA4woLC3XLLbf0u3bLLbeosLBwkCcCYBqBAsA4x3Hk9/v7XfP7/Vd8+QdA/CJQABjX2Nioo0eP9rt29OhRNTY2DvJEAEwjUAAYl5eXpylTplx2pCQpKUlTp07lJFkgAREoAIxzHEfLly+/7GJtSUlJWr58OS/xAAmIQAFghZycHJWVlUVixHEclZWV6bbbbjM8GQATCBQA1njkkUc0ZswYSVJmZqbKysoMTwTAFAIFgDW8Xq8qKirk8/n0zDPPyOv1mh4JgCEDvpsxANxMRUVFKioqMj0GAMM4ggIAAKxDoACwSiAQUGlpqQKBgOlRABhEoACwRigUUlVVlU6fPq2qqiqFQiHTIwEwhEABYI3t27ervb1dktTe3q6amhrDEwEwhUABYIXm5mbV1NRE7mrsuq5qamrU3NxseDIAJhAoAIxzXVcbNmy44vYL0QIgcRAoAIxrbGxUfX29ent7o7b39vaqvr6emwUCCYhAAWDchZsF9oebBQKJiUABYJzjOCotLe13rbS0lJsFAgmIQAFgnOu62rJlS79r1dXVnIMCJCACBYBxJ0+e1OHDh/tdO3z4sE6ePDnIEwEwjUABAADWIVAAGJefn68JEyb0u/ajH/1I+fn5gzwRANMIFABW8Hg8/W5PTU0d5EkA2IBAAWBcY2PjVc9B4TooQOIhUAAYl5OTo+Tk5H7XkpOTlZOTM8gTATCNQAFg3MGDBy+7iuwFvb29Onjw4CBPBMA0AgWAcVOnTlVSUv//HSUlJWnq1KmDPBEA0wgUAMY1Njaqr6+v37W+vj7OQQESEIECwLhTp07FtA4g/hAoAIzz+/0xrQOIPwQKAONuv/32q16o7fbbbx/cgQAYR6AAMM5xHD3xxBP9rj3xxBPczRhIQAQKAONc19XmzZv7XXv99de5mzGQgAgUAMadOHFCR48e7Xft6NGjOnHixOAOBMA4AgWAcbyLB8ClCBQAxvEuHgCXIlAAGJefny+v19vvmtfrVX5+/iBPBMA0AgWAcY2NjQqFQv2uhUIhriQLJCACBQAAWIdAAWBcTk5OTOsA4g+BAsC4Dz74IKZ1APGHQAFg3IMPPhjTOoD4Q6AAMK65uTmmdQDxh0ABYNy1LmXPpe6BxEOgAAAA6xAoAIy71t2KuZsxkHgGHCgff/yx5s2bJ7/fL8dx9O6770atu66r1atXy+/3a/jw4ZoxY4a++uqrqH3C4bCWLVumzMxMjRw5UvPnz+c1ZiCB5ebmxrQOIP4MOFDOnj2ru+66Sxs3bux3/ZVXXlFVVZU2btyo+vp6ZWVl6f7771dnZ2dkn/Lycu3atUs7duzQgQMH1NXVpblz56q3t/f6nwmAIevvf/97TOsA4k/KQD+hpKREJSUl/a65rqv169frhRde0MMPPyxJ2rJli3w+n2pqavTkk08qGAzqjTfe0LZt2zR79mxJ0ltvvaXc3Fzt3btXc+bMieHpABiKDh48eM31e++9d5CmAWCDG3oOyvHjx9Xa2qri4uLINo/Ho/vuu0+BQECS1NDQoHPnzkXt4/f7VVBQENnnUuFwWB0dHVEPAPFj6tSpMa0DiD83NFBaW1slST6fL2q7z+eLrLW2tio1NVWjRo264j6XqqysVEZGRuTB69FAfJk+fbqSkvr/7ygpKUnTp08f5IkAmHZT3sVz6Rn3rute8yz8q+2zatUqBYPByKOpqemGzQrAvL6+PvX19Q14DUD8uqGBkpWVJUmXHQlpa2uLHFXJyspST0+Pzpw5c8V9LuXxeJSenh71ABA/tmzZEtM6gPhzQwNl3LhxysrKUl1dXWRbT0+P9u/fr6KiIknSpEmTNGzYsKh9WlpadOTIkcg+ABLL+PHjY1oHEH8G/C6erq4u/fvf/458fPz4cR06dEijR49WXl6eysvLtXbtWo0fP17jx4/X2rVrNWLECJWVlUmSMjIytHjxYq1YsUJjxozR6NGjtXLlSk2cODHyrh4AiSU7OzumdQDxZ8CB8vnnn2vmzJmRjysqKiRJjz32mKqrq/Xss8+qu7tbTz/9tM6cOaPCwkLt2bNHaWlpkc9Zt26dUlJStHDhQnV3d2vWrFmqrq5WcnLyDXhKAIaaTz755JrrHEUBEovjDsG7cHV0dCgjI0PBYJDzUYA40NPTE3XpgUvt2bNHqampgzgRgJthID+/uRcPAOM+//zzmNYBxB8CBYBxF94BeL3rAOIPgQIAAKxDoAAwrqWlJaZ1APGHQAFgnN/vj2kdQPwhUAAYN3bs2JjWAcQfAgWAcb/5zW9iWgcQfwgUAMY9//zzMa0DiD8ECgDj/vvf/8a0DiD+ECgAjHMcJ6Z1APGHQAFg3OTJk2NaBxB/CBQAxm3YsCGmdQDxh0ABYNy3334b0zqA+EOgADCur68vpnUA8YdAAWBcampqTOsA4g+BAsC4ioqKmNYBxB8CBYBxTz31VEzrAOIPgQLAuM7OzpjWAcQfAgWAcSNHjoxpHUD8IVAAGHfu3LmY1gHEHwIFgHGvv/56TOsA4g+BAsC48vLymNYBxB8CBYBxbW1tMa0DiD8ECgDjzp8/H9M6gPhDoAAwjivJArgUgQLAuOrq6pjWAcQfAgWAcY8//nhM6wDiD4ECwLizZ8/GtA4g/hAoAADAOgQKAACwDoECwLiXX345pnUA8YdAAWDcc889F9M6gPhDoAAAAOsQKAAAwDoECgAAsA6BAsC4559/PqZ1APGHQAFg3B/+8IeY1gHEHwIFAABYh0ABAADWIVAAAIB1CBQAxs2YMSOmdQDxh0ABYNy+fftiWgcQfwgUAABgHQIFAABYh0ABAADWIVAAGPfjH/84pnUA8YdAAWDcl19+GdM6gPhDoAAAAOsQKAAAwDoECgAAsA6BAgAArJNyo7/g6tWrtWbNmqhtPp9Pra2tkiTXdbVmzRpt3rxZZ86cUWFhof785z/rzjvvvNGjAN+L67oKhUKmx8A1dHd3mx4hoXm9XjmOY3oMJJAbHiiSdOedd2rv3r2Rj5OTkyN/fuWVV1RVVaXq6mpNmDBBL730ku6//359/fXXSktLuxnjAFcVCoVUUlJiegxcA/9GZtXW1mr48OGmx0ACuSkv8aSkpCgrKyvyuPXWWyV995vq+vXr9cILL+jhhx9WQUGBtmzZov/973+qqam5GaMAAIAh6KYcQTl27Jj8fr88Ho8KCwu1du1a/eAHP9Dx48fV2tqq4uLiyL4ej0f33XefAoGAnnzyyZsxDnBVXq9XtbW1psdIeFc7QsK/j3ler9f0CEgwNzxQCgsLtXXrVk2YMEGnT5/WSy+9pKKiIn311VeR81B8Pl/U5/h8Pp08efKKXzMcDiscDkc+7ujouNFjI4E5jsOhawvs27dPM2bM6Hc7gMRzwwPl4t+CJk6cqLvvvls//OEPtWXLFk2bNk2SLjvRynXdq558VVlZedmJtwAAIH7d9LcZjxw5UhMnTtSxY8eUlZUlSZEjKRe0tbVddlTlYqtWrVIwGIw8mpqaburMAMy49KUcjp4AieumB0o4HNa//vUvZWdna9y4ccrKylJdXV1kvaenR/v371dRUdEVv4bH41F6enrUA0B847wTILHd8Jd4Vq5cqXnz5ikvL09tbW166aWX1NHRoccee0yO46i8vFxr167V+PHjNX78eK1du1YjRoxQWVnZjR4FAAAMUTc8UJqbm/XLX/5S33zzjW699VZNmzZNn376qfLz8yVJzz77rLq7u/X0009HLtS2Z88eroECAAAiHNd1XdNDDFRHR4cyMjIUDAZ5uQeII93d3ZET7bkwGBB/BvLzm3vxAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6xAoAADAOgQKAACwDoECAACsQ6AAAADrECgAAMA6BAoAALBOiukBEpXrugqFQqbHAKxy8fcE3x9A/7xerxzHMT3GTUegGBIKhVRSUmJ6DMBaCxYsMD0CYKXa2loNHz7c9Bg3HS/xAAAA63AExQJd/+eXcpP4pwDkulLf+e/+nJQiJcBhbOD7cPrO65ZDb5seY1DxU9ECblKKlDzM9BiAJVJNDwBYxzU9gAG8xAMAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArEOgAAAA6/A2Y0Nc96I3jfWeMzcIAMB+F/2ciPr5EccIFEPC4XDkz2n/b4fBSQAAQ0k4HNaIESNMj3HT8RIPAACwDkdQDPF4PJE/d971f7mSLADgynrPRY62X/zzI54RKIZE3So7eRiBAgD4XpwEuUcVL/EAAADrECgAAMA6BAoAALAOgQIAAKzDSbIWcPrOKzEuuwNcg+tKfee/+3NSipQgJwMC1+Jc+L5IIASKBW459LbpEQAAsAov8QAAAOtwBMUQr9er2tpa02MAVgmFQlqwYIEkadeuXfJ6vYYnAuyTKN8XBIohjuNo+PDhpscArOX1evkeARIYL/EAAADrECgAAMA6BAoAALAOgQIAAKxDoAAAAOsQKAAAwDoECgAAsA6BAgAArGP0Qm2vvfaa/vjHP6qlpUV33nmn1q9fr3vuucfkSEhArusqFAqZHgNS1L8D/yZ28Xq9crh5IwaRsUDZuXOnysvL9dprr+mnP/2pXn/9dZWUlOif//yn8vLyTI2FBBQKhVRSUmJ6DFziwiXvYYfa2lqu7ItBZewlnqqqKi1evFi//vWvdccdd2j9+vXKzc3Vpk2bTI0EAAAsYeQISk9PjxoaGvT8889HbS8uLlYgELhs/3A4rHA4HPm4o6Pjps+IxMGNG+3hum7ke93j8fCSgkUS5QZ1sIeRQPnmm2/U29srn88Xtd3n86m1tfWy/SsrK7VmzZrBGg8Jhhs32mXEiBGmRwBgAaPv4rn0tyPXdfv9jWnVqlUKBoORR1NT02CNCAAADDByBCUzM1PJycmXHS1pa2u77KiK9N2hXo/HM1jjAQAAw4wcQUlNTdWkSZNUV1cXtb2urk5FRUUmRgIAABYx9jbjiooKPfroo5o8ebLuvvtubd68WY2NjXrqqadMjQQAACxhLFBKS0vV3t6uF198US0tLSooKNDu3buVn59vaiQAAGAJx3Vd1/QQA9XR0aGMjAwFg0Glp6ebHgcAAHwPA/n5zb14AACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFjH2IXaYnHh0i0dHR2GJwEAAN/XhZ/b3+cSbEMyUDo7OyVJubm5hicBAAAD1dnZqYyMjKvuMySvJNvX16dTp04pLS1NjuOYHgfADdTR0aHc3Fw1NTVxpWggzriuq87OTvn9fiUlXf0skyEZKADiF7eyACBxkiwAALAQgQIAAKxDoACwisfj0e9//3t5PB7TowAwiHNQAACAdTiCAgAArEOgAAAA6xAoAADAOgQKAACwDoECwCqvvfaaxo0bJ6/Xq0mTJumTTz4xPRIAAwgUANbYuXOnysvL9cILL+iLL77QPffco5KSEjU2NpoeDcAg423GAKxRWFion/zkJ9q0aVNk2x133KGHHnpIlZWVBicDMNg4ggLACj09PWpoaFBxcXHU9uLiYgUCAUNTATCFQAFghW+++Ua9vb3y+XxR230+n1pbWw1NBcAUAgWAVRzHifrYdd3LtgGIfwQKACtkZmYqOTn5sqMlbW1tlx1VARD/CBQAVkhNTdWkSZNUV1cXtb2urk5FRUWGpgJgSorpAQDggoqKCj366KOaPHmy7r77bm3evFmNjY166qmnTI8GYJARKACsUVpaqvb2dr344otqaWlRQUGBdu/erfz8fNOjARhkXAcFAABYh3NQAACAdQgUAABgHQIFAABYh0ABAADWIVAAAIB1CBQAAGAdAgUAAFiHQAEAANYhUAAAgHUIFAAAYB0CBQAAWIdAAQAA1vn/zseg4+owa1kAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99a25a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64626d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  6423 24231\n",
      "     2  7652   102   170     2    47  1898     1   269    17     7 13230\n",
      "   619  1711  2761     3] \n",
      "\n",
      "\n",
      "\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  1  3 11  1  6  3  2  2  5  1  4  5  6\n",
      "  1  1 11  3]\n"
     ]
    }
   ],
   "source": [
    "# print the first sequence\n",
    "print(X_padded[0], \"\\n\"*3)\n",
    "print(Y_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c23b3b",
   "metadata": {},
   "source": [
    "RNN will learn the zero to zero mapping while training. So we don't need to worry about the padded zeroes. Please note that zero is not reserved for any word or tag, it's only reserved for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "572d9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign padded sequences to X and Y\n",
    "X, Y = X_padded, Y_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e690e29",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94693d0c",
   "metadata": {},
   "source": [
    "Currently, each word and each tag is encoded as an integer. \n",
    "\n",
    "We'll use a more sophisticated technique to represent the input words (X) using what's known as **word embeddings**.\n",
    "\n",
    "However, to represent each tag in Y, we'll simply use **one-hot encoding** scheme since there are only 13 tags in the dataset and the LSTM will have no problems in learning its own representation of these tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919eb888",
   "metadata": {},
   "source": [
    "To use word embeddings, you can go for either of the following models:\n",
    "1. word2vec model: https://code.google.com/archive/p/word2vec/\n",
    "2. GloVe model : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "We're using the word2vec model for no particular reason. Both of these are very efficient in representing words. You can try both and see which one works better.\n",
    "\n",
    "Dimensions of a word embedding is: (VOCABULARY_SIZE, EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97215e3",
   "metadata": {},
   "source": [
    "### Use word embeddings for input sequences (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "875d1f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'word-embeddings/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword-embeddings/GoogleNews-vectors-negative300.bin.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load word2vec using the following function present in the gensim library\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m word2vec \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(path, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39mfvocab, binary\u001b[38;5;241m=\u001b[39mbinary, encoding\u001b[38;5;241m=\u001b[39mencoding, unicode_errors\u001b[38;5;241m=\u001b[39municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit, datatype\u001b[38;5;241m=\u001b[39mdatatype, no_header\u001b[38;5;241m=\u001b[39mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:235\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(ve\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 235\u001b[0m binary \u001b[38;5;241m=\u001b[39m _open_binary_stream(uri, binary_mode, transport_params)\n\u001b[1;32m    236\u001b[0m decompressed \u001b[38;5;241m=\u001b[39m so_compression\u001b[38;5;241m.\u001b[39mcompression_wrapper(binary, binary_mode, compression)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mor\u001b[39;00m explicit_encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/smart_open_lib.py:398\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    396\u001b[0m scheme \u001b[38;5;241m=\u001b[39m _sniff_scheme(uri)\n\u001b[1;32m    397\u001b[0m submodule \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mget_transport(scheme)\n\u001b[0;32m--> 398\u001b[0m fobj \u001b[38;5;241m=\u001b[39m submodule\u001b[38;5;241m.\u001b[39mopen_uri(uri, mode, transport_params)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    400\u001b[0m     fobj\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m uri\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/smart_open/local_file.py:34\u001b[0m, in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_uri\u001b[39m(uri_as_string, mode, transport_params):\n\u001b[1;32m     33\u001b[0m     parsed_uri \u001b[38;5;241m=\u001b[39m parse_uri(uri_as_string)\n\u001b[0;32m---> 34\u001b[0m     fobj \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(parsed_uri[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muri_path\u001b[39m\u001b[38;5;124m'\u001b[39m], mode)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word-embeddings/GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# word2vec download link (Size ~ 1.5GB): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "path = 'word-embeddings/GoogleNews-vectors-negative300.bin.gz'\n",
    "\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd7ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
